{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce9702be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS inicializado - Vectores: 79044 | Dimensi칩n: 768\n"
     ]
    }
   ],
   "source": [
    "from get_response import PoliGPT\n",
    "import rag_metrics as rm\n",
    "import json\n",
    "\n",
    "poligpt = PoliGPT(faiss_index_dir='../01_data/project_faiss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34aff15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(json_path):\n",
    "    preguntas, respuestas_modelo = [], []\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for entry in data:\n",
    "        for preg in entry[\"preguntas\"]:\n",
    "            preguntas.append(preg[\"pregunta\"])\n",
    "            respuestas_modelo.append(preg[\"respuesta\"])\n",
    "\n",
    "    return preguntas, respuestas_modelo\n",
    "\n",
    "preguntas, respuestas_modelo = read_json(\"../01_data/preguntas.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a48f11bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "respuestas = []\n",
    "for pregunta in preguntas:  \n",
    "    respuesta = poligpt.query_poligpt(pregunta, k_context=2)\n",
    "    if respuesta['response'] != 'No dispongo de informaci칩n suficiente en el contexto proporcionado.':\n",
    "        respuestas.append(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "861183c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_context_data(respuestas):\n",
    "    \"\"\"\n",
    "    Toma la salida de PoliGPT (lista de dicts con 'context_used') y\n",
    "    devuelve:\n",
    "      - all_contexts: para cada respuesta, la lista de textos de contexto.\n",
    "      - all_metadata: para cada respuesta, la lista de tuplas (doc_id, p치gina).\n",
    "    \"\"\"\n",
    "    all_contexts = []\n",
    "    all_metadata = []\n",
    "    for rsp in respuestas:\n",
    "        ctxs = []\n",
    "        metas = []\n",
    "        for doc, score in rsp['context_used']:\n",
    "            # doc es un objeto Document con atributos .metadata y .id\n",
    "            ctxs.append(doc.page_content)\n",
    "            # extraemos el id y la p치gina:\n",
    "            doc_id = getattr(doc, 'id', None)\n",
    "            page   = doc.metadata.get('page')\n",
    "            metas.append((doc_id, page))\n",
    "        all_contexts.append(ctxs)\n",
    "        all_metadata.append(metas)\n",
    "    return all_contexts, all_metadata\n",
    "\n",
    "contextos, metadatos = extract_context_data(respuestas)\n",
    "respuestas_final = [dic['response'] for dic in respuestas]\n",
    "preguntas_final = [dic['query'] for dic in respuestas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d70a2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataset(preguntas, respuestas, contextos):\n",
    "    \"\"\"Aggregate the three metrics over an iterable of RAG records.\"\"\"\n",
    "    g_scores, f1s, qc_sims = [], [], []\n",
    "    for answer, contexts, question in zip(respuestas, contextos,preguntas):\n",
    "        g_scores.append(rm.grounding_score(answer, contexts))\n",
    "        f1s.append(rm.context_overlap_f1(answer, contexts))\n",
    "        qc_sims.append(rm.question_context_similarity(question, contexts))\n",
    "    n = max(len(g_scores), 1)\n",
    "    return {\n",
    "        \"Grounding\": sum(g_scores) / n,\n",
    "        \"ContextOverlapF1\": sum(f1s) / n,\n",
    "        \"QuestionContextSim\": sum(qc_sims) / n,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82faef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_references(preguntas, respuestas, contextos, preguntas_referencia):\n",
    "    \"\"\"Aggregate both reference-based and reference-free metrics.  \n",
    "    Records must include keys:\n",
    "      - \"respuesta\" (prediction)\n",
    "      - \"reference\" (ground truth answer)\n",
    "      - \"pregunta\", \"contextos\"\n",
    "    \"\"\"\n",
    "    ems, f1_r, rouge_l, gs, f1s, sims = [], [], [], [], [], []\n",
    "    for pred, ref, ctx, q in zip(respuestas, preguntas_referencia, contextos, preguntas):\n",
    "        ems.append(rm.exact_match_score(pred, ref))\n",
    "        f1_r.append(rm.token_f1_score(pred, ref))\n",
    "        rouge_l.append(rm.rouge_l_score(pred, ref))\n",
    "        gs.append(rm.grounding_score(pred, ctx))\n",
    "        f1s.append(rm.context_overlap_f1(pred, ctx))\n",
    "        sims.append(rm.question_context_similarity(q, ctx))\n",
    "    n = max(len(ems), 1)\n",
    "    return {\n",
    "        \"ExactMatch\": sum(ems)/n,\n",
    "        \"TokenF1\":    sum(f1_r)/n,\n",
    "        \"ROUGE_L\":    sum(rouge_l)/n,\n",
    "        \"GroundingScore\":  sum(gs)/n,\n",
    "        \"ContextOverlapF1\":  sum(f1s)/n,\n",
    "        \"QuestionContextSim\": sum(sims)/n\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b09d836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Grounding': 0.585236022822446,\n",
       " 'ContextOverlapF1': 0.3302083655276197,\n",
       " 'QuestionContextSim': 0.14030019196411614}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_dataset(preguntas_final, respuestas_final, contextos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18c876c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ExactMatch': 0.0,\n",
       " 'TokenF1': 0.16386001907811462,\n",
       " 'ROUGE_L': 0.13400927280945793,\n",
       " 'Grounding': 0.585236022822446,\n",
       " 'ContextF1': 0.3302083655276197,\n",
       " 'Ques-ContextSim': 0.14030019196411614}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_with_references(preguntas_final, respuestas_final, contextos, respuestas_modelo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
