{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9702be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS inicializado - Vectores: 139471 | Dimensi칩n: 768\n"
     ]
    }
   ],
   "source": [
    "from get_response import PoliGPT\n",
    "import rag_metrics as rm\n",
    "import json\n",
    "\n",
    "poligpt = PoliGPT(faiss_index_dir='../01_data/project_faiss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34aff15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(json_path):\n",
    "    preguntas, respuestas_modelo = [], []\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for entry in data:\n",
    "        for preg in entry[\"preguntas\"]:\n",
    "            preguntas.append(preg[\"pregunta\"])\n",
    "            respuestas_modelo.append(preg[\"respuesta\"])\n",
    "\n",
    "    return preguntas, respuestas_modelo\n",
    "\n",
    "preguntas, respuestas_modelo = read_json(\"../01_data/preguntas.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48f11bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "respuestas = []\n",
    "for pregunta in preguntas:  \n",
    "    respuesta = poligpt.query_poligpt(pregunta, k_context=2)\n",
    "    if \"error\" in respuesta:\n",
    "        print(pregunta)\n",
    "        continue\n",
    "    elif respuesta['response'] != 'No dispongo de informaci칩n suficiente en el contexto proporcionado.':\n",
    "        respuestas.append(respuesta)\n",
    "\n",
    "    else:\n",
    "        pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2677776a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuestas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "861183c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_context_data(respuestas):\n",
    "    \"\"\"\n",
    "    Toma la salida de PoliGPT (lista de dicts con 'context_used') y\n",
    "    devuelve:\n",
    "      - all_contexts: para cada respuesta, la lista de textos de contexto.\n",
    "      - all_metadata: para cada respuesta, la lista de tuplas (doc_id, p치gina).\n",
    "    \"\"\"\n",
    "    all_contexts = []\n",
    "    all_metadata = []\n",
    "    for rsp in respuestas:\n",
    "        ctxs = []\n",
    "        metas = []\n",
    "        for doc, score in rsp['context_used']:\n",
    "            # doc es un objeto Document con atributos .metadata y .id\n",
    "            ctxs.append(doc.page_content)\n",
    "            # extraemos el id y la p치gina:\n",
    "            doc_id = getattr(doc, 'id', None)\n",
    "            page   = doc.metadata.get('page')\n",
    "            metas.append((doc_id, page))\n",
    "        all_contexts.append(ctxs)\n",
    "        all_metadata.append(metas)\n",
    "    return all_contexts, all_metadata\n",
    "\n",
    "contextos, metadatos = extract_context_data(respuestas)\n",
    "respuestas_final = [dic['response'] for dic in respuestas]\n",
    "preguntas_final = [dic['query'] for dic in respuestas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d70a2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataset(preguntas, respuestas, contextos):\n",
    "    \"\"\"Aggregate the three metrics over an iterable of RAG records.\"\"\"\n",
    "    g_scores, f1s, qc_sims = [], [], []\n",
    "    for answer, contexts, question in zip(respuestas, contextos,preguntas):\n",
    "        g_scores.append(rm.grounding_score(answer, contexts))\n",
    "        f1s.append(rm.context_overlap_f1(answer, contexts))\n",
    "        qc_sims.append(rm.question_context_similarity(question, contexts))\n",
    "    n = max(len(g_scores), 1)\n",
    "    return {\n",
    "        \"Grounding\": sum(g_scores) / n,\n",
    "        \"ContextOverlapF1\": sum(f1s) / n,\n",
    "        \"QuestionContextSim\": sum(qc_sims) / n,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82faef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_references(preguntas, respuestas, contextos, preguntas_referencia, model_name):\n",
    "    \"\"\"Aggregate both reference-based and reference-free metrics.  \n",
    "    Records must include keys:\n",
    "      - \"respuesta\" (prediction)\n",
    "      - \"reference\" (ground truth answer)\n",
    "      - \"pregunta\", \"contextos\"\n",
    "    \"\"\"\n",
    "    ems, f1_r, rouge_l, gs, f1s, sims, c_cxt_anw, c_qst_anw, c_qst_cxt, l2_cxt_anw, l2_qst_anw, l2_qst_cxt, c_ref_anw, l2_ref_anw = [], [], [], [], [], [], [], [], [], [], [], [], [], []\n",
    "    for pred, ref, ctx, q in zip(respuestas, preguntas_referencia, contextos, preguntas):\n",
    "        ems.append(rm.exact_match_score(pred, ref))\n",
    "        f1_r.append(rm.token_f1_score(pred, ref))\n",
    "        rouge_l.append(rm.rouge_l_score(pred, ref))\n",
    "        gs.append(rm.grounding_score(pred, ctx))\n",
    "        f1s.append(rm.context_overlap_f1(pred, ctx))\n",
    "        sims.append(rm.question_context_similarity(q, ctx))\n",
    "        c_cxt_anw.append(rm.cosine_similarity_score_context(pred, ctx, model_name))\n",
    "        c_qst_anw.append(rm.cosine_similarity_score(pred, q, model_name))\n",
    "        c_qst_cxt.append(rm.cosine_similarity_score_context(q, ctx, model_name))\n",
    "        c_ref_anw.append(rm.cosine_similarity_score(ref, pred, model_name))\n",
    "        l2_cxt_anw.append(rm.avg_l2_distance_context(pred, ctx, model_name))\n",
    "        l2_qst_anw.append(rm.avg_l2_distance(pred, q, model_name))\n",
    "        l2_qst_cxt.append(rm.avg_l2_distance_context(q, ctx, model_name))\n",
    "        l2_ref_anw.append(rm.avg_l2_distance(ref, pred, model_name))\n",
    "\n",
    "    n = max(len(ems), 1)\n",
    "    return {\n",
    "        \"ExactMatch\": sum(ems)/n,\n",
    "        \"TokenF1\":    sum(f1_r)/n,\n",
    "        \"ROUGE_L\":    sum(rouge_l)/n,\n",
    "        \"GroundingScore\":  sum(gs)/n,\n",
    "        \"ContextOverlapF1\":  sum(f1s)/n,\n",
    "        \"QuestionContextSim\": sum(sims)/n,\n",
    "        \"CosineContextAnswer\": sum(c_cxt_anw)/n,\n",
    "        \"CosineQuestionAnswer\": sum(c_qst_anw)/n,\n",
    "        \"CosineQuestionContext\": sum(c_qst_cxt)/n,\n",
    "        \"CosineReferenceAnswer\": sum(c_ref_anw)/n,\n",
    "        \"L2ContextAnswer\": sum(l2_cxt_anw)/n,\n",
    "        \"L2QuestionAnswer\": sum(l2_qst_anw)/n,\n",
    "        \"L2QuestionContext\": sum(l2_qst_cxt)/n,\n",
    "        \"L2ReferenceAnswer\": sum(l2_ref_anw)/n,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddfb8585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 6 6\n"
     ]
    }
   ],
   "source": [
    "print(len(preguntas_final), len(respuestas_final), len(contextos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b09d836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Grounding': 0.6742382380292731,\n",
       " 'ContextOverlapF1': 0.5018434104390614,\n",
       " 'QuestionContextSim': 0.1981582161117383}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_dataset(preguntas_final, respuestas_final, contextos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18c876c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ExactMatch': 0.0,\n",
       " 'TokenF1': 0.06753010169702935,\n",
       " 'ROUGE_L': 0.0642938557423368,\n",
       " 'GroundingScore': 0.6742382380292731,\n",
       " 'ContextOverlapF1': 0.5018434104390614,\n",
       " 'QuestionContextSim': 0.1981582161117383,\n",
       " 'CosineContextAnswer': 0.854008803764979,\n",
       " 'CosineQuestionAnswer': 0.8704183300336202,\n",
       " 'CosineQuestionContext': 0.7997889518737793,\n",
       " 'CosineReferenceAnswer': 0.20269746085007986,\n",
       " 'L2ContextAnswer': 1.4268150726954143,\n",
       " 'L2QuestionAnswer': 1.4146085778872173,\n",
       " 'L2QuestionContext': 1.7336900035540264,\n",
       " 'L2ReferenceAnswer': 3.3856134017308555}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_with_references(preguntas_final, respuestas_final, contextos, respuestas_modelo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
